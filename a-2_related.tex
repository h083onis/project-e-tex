\section*{A-2. 関連技術}

\subsection*{2.1 BLE\ (Bluetooth Low Energy)}
BLE（Bluetooth Low Energy）は，Bluetooth規格の一部であり，低消費電力での通信を目的としている\cite{ble}．主にセンサーデバイスやウェアラブル機器，IoTデバイスに利用され，短距離無線通信を効率的に行うことが可能である．従来のBluetooth Classicと比較して消費電力が非常に少なく，バッテリー駆動のデバイスに適している\cite{ble}．また，BLEはAdvertisingとScaningの2つのモードを利用して通信を行う\cite{ble}．Advertisingはデバイスが周期的に信号を送信するモードであり，Scaningは信号を受信し解析するモードである．これにより，BLEは位置情報の測定やデバイスの存在検知に活用されている．

\subsection*{2.2 RSSI（Received Signal Strength Indicator）とMACアドレス}
RSSI（Received Signal Strength Indicator）は，受信信号強度を示す指標であり，無線通信において受信側のデバイスが測定する信号の強度を示す値である\cite{ble}．一般に，RSSIは負のdBm（デシベルミリワット）単位で表され，値が0に近いほど信号が強く，負の値が大きくなるほど信号が弱いことを意味する．

RSSIは，無線通信の品質評価や通信経路の選択，位置推定など，さまざまな用途で利用される．特に位置推定では，複数のアクセスポイントや基地局からのRSSI値を用いて，電波の減衰特性に基づいた推定を行う手法が一般的である．RSSIの測定は追加のハードウェアは必要とせず，ほとんどの無線通信モジュールに標準的に搭載されているため，位置情報の取得においてコスト効率の良い手法とされている．

しかし，RSSIの測定値は周囲の環境要因に大きく影響される\cite{ble}．例えば，電波の反射，回折，散乱，遮蔽物の存在などが測定値に誤差を生じさせる．特に屋内環境ではマルチパス伝播の影響が顕著であり，精度の低下を引き起こす．また，異なるデバイスやチップセットによってRSSIの測定精度や値の補正方法が異なるため，環境に依存しない正確な推定を行うためには，適切なキャリブレーションやフィルタリングが必要となる．


MACアドレス（Media Access Control Address）は，ネットワークインターフェースに一意に割り当てられた識別子であり，ネットワーク層でのデバイス識別や通信制御に用いられる\cite{tanenbaum}．MACアドレスは通常48ビットの長さを持ち，12桁の16進数（例：\texttt{00:1A:2B:3C:4D:5E}）で表記される．MACアドレスはOSI参照モデル\cite{tanenbaum}のデータリンク層において使用され，同一ネットワーク内での通信を確立する際の基本的な識別子として機能する．

MACアドレスは物理的にネットワーク機器に埋め込まれており，通常はデバイスの製造時に割り当てられる．最初の24ビットは製造者識別子（Organizationally Unique Identifier, OUI）として定められ，特定のメーカーに割り当てられる．残りの24ビットはメーカーが独自に管理し，各デバイスに一意となるよう設定される．この仕組みにより，世界中の全てのネットワークデバイスに対して一意のMACアドレスを付与することが可能となっている．

\subsection*{2.3 Raspberry Pi 4}
Raspberry Pi 4は，シングルボードコンピュータであり，BLE通信の受信やデータ処理を行うためのデバイスとして利用することができる\cite{rasPi}．ARM Cortex-A72プロセッサと最大8GBのRAMを搭載し，小型ながら高性能な処理が可能である\cite{rasPi}．BLE受信モジュールを搭載しており，周囲のBLEビーコンやデバイスをスキャンして信号強度指標（RSSI）やMACアドレスなどの情報を取得できる\cite{rasPi}．
BLEスキャンは，Raspberry Pi 4に搭載されたBluetooth 5.0対応モジュールを用いて行われる．Linux環境では，hcitoolやbluetoothctlといったコマンドラインツールを使用することで，BLEデバイスを検出し，一定間隔でRSSI値を取得することが可能である．
取得したRSSI値は，距離推定や位置推定の指標として使用される．
また，Raspberry Pi 4は低コストで簡単に配置できるため，複数台を使用した分散型のデータ収集環境を構築することも容易である．各デバイスが収集したデータをネットワークを介して集約・統合し，人数推定モデルの入力として利用できる．分散型環境では，各Raspberry Pi 4が独立してデータ収集を行うことで，リアルタイム性を維持しつつ効率的な処理を実現できる．

\subsection*{2.4 回帰モデル}
回帰モデルは，入力変数と出力変数の間の関係を数理的に表現するための統計的手法である\cite{prml}．特にデータの分布や傾向を捉え，未知のデータに対して，ある連続的な目的変数を予測する場合に用いられる．回帰モデルは，データの分布や傾向を捉え，未知のデータに対して予測を行うために用いられる．一般的に，回帰モデルは次の式で表される．

\begin{equation}
	y = f(\mathbf{x}) + \epsilon
\end{equation}
ここで，$y$ は目的変数（従属変数）であり，$\mathbf{x} = (x_1, x_2, \ldots, x_p)$ は $p$ 個の説明変数（独立変数）のベクトルを表す．$f(\mathbf{x})$ は説明変数と目的変数の関係を表す未知の関数であり，$\epsilon$ は誤差項である．誤差項は観測誤差やモデル化の不確実性を表し，通常は平均0，分散$\sigma^2$の正規分布に従うと仮定される．

\subsubsection*{単回帰モデル}
最も基本的な回帰モデルとして，単回帰モデルが挙げられる．単回帰モデルでは，1つの説明変数を用いて目的変数を予測する\cite{prml}．そのモデルは次の式で表される．

\begin{equation}
	y = \beta_0 + \beta_1 x + \epsilon
\end{equation}
ここで，$x$ は説明変数，$y$ は目的変数である．$\beta_0$ は切片（定数項）であり，説明変数が0のときの目的変数の期待値を表す．$\beta_1$ は回帰係数（傾き）で，説明変数が1単位増加したときに目的変数がどの程度変化するかを示す．$\epsilon$ は誤差項であり，観測値とモデルの予測値の差を含む．

\subsubsection*{重回帰モデル}
複数の説明変数を用いる場合，重回帰モデルが使用される\cite{prml}．重回帰モデルは次のように表される．

\begin{equation}
	y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon
\end{equation}
ここで，$x_1, x_2, \ldots, x_p$ は $p$ 個の説明変数，$\beta_0$ は切片，$\beta_1, \beta_2, \ldots, \beta_p$ は各説明変数に対する回帰係数である．誤差項 $\epsilon$ は，単回帰モデルと同様に観測誤差やモデル化の不確実性を表す．

\subsubsection*{最小二乗法}
回帰モデルの係数 $\beta_0, \beta_1, \ldots, \beta_p$ を推定するためには，一般に最小二乗法が用いられる\cite{prml}．最小二乗法では，観測値とモデルの予測値の差の二乗和を最小化することを目的とする．損失関数（残差平方和）は次のように定義される．

\begin{equation}
	L(\beta_0, \beta_1, \ldots, \beta_p) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2
\end{equation}
ここで，$n$ はサンプルサイズ，$y_i$ は $i$ 番目の観測値，$\hat{y}_i$ はモデルによって予測された $i$ 番目の値である．$x_{ij}$ は $i$ 番目のサンプルの $j$ 番目の説明変数を示す．損失関数 $L$ を最小化することで，最適な回帰係数を求めることができる．

最小二乗法の解は以下の正規方程式を解くことで得られる．

\begin{equation}
	(\mathbf{X}^T \mathbf{X}) \mathbf{\beta} = \mathbf{X}^T \mathbf{y}
\end{equation}
ここで，$\mathbf{X}$ は $n \times (p+1)$ 次元の説明変数のデザイン行列であり，$\mathbf{\beta}$ は $(p+1)$ 次元の回帰係数のベクトル，$\mathbf{y}$ は $n$ 次元の目的変数のベクトルを表す．$\mathbf{X}^T$ は $\mathbf{X}$ の転置行列である．この方程式を解くことで，最小二乗推定量を得ることができる．

\subsubsection*{SVR（Suuport Vector Regression）}
SVRは，サポートベクターマシン（SVM）の枠組みに基づく回帰手法であり，高次元空間における非線形な関係も捉えることが可能である\cite{prml}．SVRでは，予測値が実際の値からある許容範囲（$\epsilon$）以内に収まるような関数を学習する．このとき，モデルの複雑さを抑えるために，関数の平滑性も同時に最適化の対象とする．SVRの回帰関数は，次のように表される．

\begin{equation}
	f(\mathbf{x}) = \langle \mathbf{w}, \phi(\mathbf{x}) \rangle + b
\end{equation}
ここで，$\mathbf{w}$ は重みベクトル，$b$ はバイアス項，$\phi(\mathbf{x})$ は入力ベクトル $\mathbf{x}$ を高次元空間に写像する特徴変換関数である．SVRでは以下の最適化問題を解くことで回帰関数を学習する．

\begin{align}
	\min_{\mathbf{w}, b, \xi_i, \xi_i^*} \quad & \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{n} (\xi_i + \xi_i^*) \\
	\text{subject to} \quad 
	& y_i - \langle \mathbf{w}, \phi(\mathbf{x}_i) \rangle - b \leq \epsilon + \xi_i \\
	& \langle \mathbf{w}, \phi(\mathbf{x}_i) \rangle + b - y_i \leq \epsilon + \xi_i^* \\
	& \xi_i \geq 0, \quad \xi_i^* \geq 0, \quad i = 1, \ldots, n
\end{align}
ここで，$C$ は誤差に対するペナルティの重みを調整する正則化パラメータであり，大きな値をとると予測誤差に対して厳しくなり，小さな値をとると,
ある程度の誤差を許容するようになる．$\xi_i$ および $\xi_i^*$ は，実際の値と予測値の差が許容範囲 $\epsilon$ を超えた場合に，その超過分を補うために導入される変数（スラック変数）である．このスラック変数の値が$0$であれば，予測誤差は $\epsilon$ の範囲内に収まっていることを意味する．

また，SVRでは入力データを高次元空間に写像して回帰を行うが，このときカーネルトリックと呼ばれる手法を用いることで，変換後の特徴量 $\phi(\mathbf{x})$ を明示的に計算せずに内積の計算が可能となる．これにより，非線形な関係を効率的に捉えることができる．よく使われるカーネル関数には，単純な関係を表す線形カーネル，非線形な関係に対応する多項式カーネル，滑らかで複雑な関係を捉えるのに適したRBF（Radial Basis Function）カーネルなどがある．

\subsection*{2.5 決定木とそのモデル}
決定木は，分類および回帰のための機械学習モデルの一つであり，データを階層的に分割することで予測を行う手法である\cite{prml}．木構造を持つこのモデルは，根ノードから始まり，内部ノードで特徴量に基づいた条件分岐を行い，最終的に葉ノードに到達して予測結果を出力する．

決定木は直感的に理解しやすく，可視化が容易であるため，特徴量の重要度を評価する際にも有用である．特に，非線形な関係や複雑なデータに対しても効果的に学習を行うことができる．

決定木における分割は，各ノードにおいて目的変数と特徴量の関係を最もよく説明できる条件を選択することで行われる．分類問題の場合はジニ不純度 (Gini Impurity) \cite{gini}やエントロピー，回帰問題の場合は分散の減少量が評価基準として用いられる．

例えば，ノード $t$ におけるジニ不純度 $G(t)$ は次の式で表される．

\begin{equation}
	G(t) = 1 - \sum_{k=1}^{K} p_k^2
\end{equation}
ここで，$K$ はクラスの数，$p_k$ はクラス $k$ に属するサンプルの割合を表す．エントロピー$H(t)$は以下のように表される．

\begin{equation}
	H(t) = - \sum_{k=1}^{K} p_k \log p_k
\end{equation}
回帰問題においては，ノード $t$ の分散 $V(t)$ を用いて評価を行う．

\begin{equation}
	V(t) = \frac{1}{N_t} \sum_{i \in t} (y_i - \bar{y}_t)^2
\end{equation}
ここで，$N_t$ はノード $t$ 内のサンプル数，$y_i$ は $i$ 番目の目的変数，$\bar{y}_t$ はノード $t$ の平均値である．


決定木は単体で使用されることも多いが，アンサンブル学習の手法においても重要な役割を果たしている．以下に，決定木と深く関連する手法をいくつか挙げる．


\subsubsection*{Random Forest}
複数の決定木を組み合わせて予測を行う手法である\cite{randomforest}．各決定木は，異なるデータサンプルで学習され，最終的な予測は各木の予測結果の平均または多数決に基づいて行われる．これにより，過学習のリスクを減少させ，高い汎化性能を得ることができる．

\subsubsection*{XGBoost}
予測の誤りを少しずつ修正しながら，複数の決定木を順番に追加していく手法（ブースティング）に基づいている\cite{xgboost}．最初の木でうまく予測できなかった部分を，次の木が重点的に学習するように工夫されている．XGBoostはこのブースティングを効率よく実行するためのアルゴリズムであり，学習の速さと精度の高さから，さまざまな分野で広く使われている．

\subsubsection*{LightGBM}
XGBoostと同様にブースティングの考え方に基づくが，特に大量のデータに対して効率的に学習できるように設計されている\cite{lightgbm}．一般的な方法では，決定木を上から順に分割していくが，LightGBMでは，より精度が高くなるような葉の部分（木の末端）を優先して分割する方法を採用している．これにより，学習時間を短縮しつつ，高い予測性能を実現している．

\subsubsection*{CatBoost}
勾配ブースティング決定木（Gradient Boosting Decision Tree）ベースの機械学習ライブラリである\cite{catboost}．カテゴリデータの処理が得意であり，高精度な予測が可能である．特に欠損値の処理や不均衡データへの対応能力が高いため，BLEのようなノイズの多いデータにも適している．
また，決定木の構築過程でカテゴリカル特徴量を最適にエンコードすることで，予測精度を向上させる．さらに，過学習を抑制する正則化手法や高速な学習アルゴリズムが組み込まれているため，大規模なBLEデータの解析にも適している．

\subsection*{2.6 BLEを用いた人数推定の先行研究}
近年，スマートフォン等のモバイルデバイスが発するBLE信号を利用した人数推定の研究が注目されている．松田ら~\cite{senkou}は，空間内に設置したBLEビーコンによって空間内に存在するBLE信号を受信し，その受信状況から公共空間内の人数を推定する手法（BLECE: BLE-based Crowdedness Estimation）を提案している．

\subsubsection*{実験対象およびデータ収集環境}

実験は以下の4つの空間で実施された：

\begin{itemize}
  \item 空間A：飲食店（カフェ，約25席，正方形）
  \item 空間B：飲食店（とんかつ店，約50席，正方形）
  \item 空間C：飲食店（とんかつ店，約50席，長方形）
  \item 空間D：公共施設（市立図書館，L字形，定員不定）
\end{itemize}

各空間には，BLECEノード（Raspberry Pi 4 Model B + BUFFALO製BLEドングル + LTEドングル）を1〜3台設置し，15秒間隔（スキャン10秒＋スリープ5秒）でBLEスキャンを実施された．スキャンにより取得されたデータは，BDアドレス（Bluetooth Device Address）およびRSSIを含むものであり，モバイル回線を通じてクラウドに送信・保存された．

人数の正解データは以下の方法で取得された：

\begin{itemize}
  \item 空間A〜C：出入口に設置した俯瞰カメラによる映像を基に5分ごとで人数を計測
  \item 空間D：調査員による15分ごとの目視計測
\end{itemize}

\subsubsection*{データ処理と特徴量設計}

BLECEノードのスキャン結果は非同期であるため，時刻を基準に直近のスキャン結果を統合し，BLEノード数に依存しない仮想的な1点のデータとして扱う．複数ノードにより同一BDアドレスが観測された場合は，最もRSSIが強い（=距離が近いと想定される）値を採用する．

特徴量は，空間内におけるBLE信号の統計的な変動，および時間帯による人流の傾向を捉えることを目的として設計されている．主な特徴量は表\ref{tbl:blece_features}に示す．

\begin{table}[tb]
	\centering
	\caption{先行研究で使用された特徴量一覧}
	\label{tbl:blece_features}
	\small
	\doublerulesep=0.3pt
    \begin{tabular}{l|p{5cm}} \hline\hline\hline
		特徴量名 & 内容 \\ \hline
		all\_num\_T\textsubscript{sec} & 過去 $T$ 秒間にスキャンされた，BD アドレスの総数\\ \hline
    unique\_num\_T\textsubscript{sec} & 過去 $T$ 秒間にスキャンされた，BD アドレスのユニーク数 \\ \hline
    unique\_ratio\_T\textsubscript{sec} & 過去 $T$ 秒間にスキャンされた，BD アドレスのうち，ユニークなデバイスの占める割合（ユニーク数 / 総数） \\ \hline
    unique\_num\_T\textsubscript{sec}\_Sdb & 過去 $T$ 秒間にスキャンされた，BD アドレスのうち，RSSI が閾値 Sdb より大きいもののユニーク数 \\ \hline\hline\hline
	\end{tabular}
\end{table}

ここで，$T$は過去の参照時間（15, 30, 45, 60秒），$S$はRSSIの閾値（--60〜--90\,dB，5\,dB刻み）であり，複数パターンを組み合わせて複数の特徴量を生成している．これにより，デバイスを所持した人の出入りや信号の断続的な観測によるノイズを軽減しつつ，空間内の人数変動を統計的に捉えることを可能にしている．

\subsubsection*{モデル構築と評価}

人数推定には，以下の3つの回帰モデルが用いられた：

\begin{itemize}
  \item Support Vector Regressor（SVR）
  \item Random Forest Regressor（RFR）
  \item XGBoost Regressor（XGBR）
\end{itemize}

各モデルのハイパーパラメータはGridSearchCVにより最適化され，3分割交差検証によって評価された．評価指標には以下の3種が用いられている：

\begin{itemize}
  \item 平均絶対誤差（MAE: Mean Absolute Error）
  \item 平均絶対パーセント誤差（MAPE: Mean Absolute Percentage Error）
  \item 二乗平均平方根誤差（RMSE: Root Mean Squared Error）
\end{itemize}

実験の結果，XGBRは全体的に最も良好な性能を示し，最大でMAE 4.59人，MAPE 58.1\%，RMSE 5.94人を記録した．特に，ユニークアドレス数やRSSIの強い信号に注目した特徴量（例：\texttt{unique\_num\_30sec\_70db}）がモデルの予測性能に大きく寄与していることが，特徴量重要度の分析から示されていた．
